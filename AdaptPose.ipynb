{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgholamikn/AdaptPose/blob/main/AdaptPose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtvy95RHLbrd"
      },
      "source": [
        "# **AdaptPose: Cross-dataset Adaptation of 3D Human Pose Estimation by Learnable Motion Generator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hfK6GfCJ5r4"
      },
      "source": [
        "Here we provide the following experiments: \n",
        "\n",
        "1.   Cross-dataset evaluation on 3DHP: source H3.6M, target: 3DHP\n",
        "2.   Cross-dataset evaluation on 3DPW: source H3.6M, target: 3DPW\n",
        "3.   Cross-dataset trainin on 3DHP: source H3.6M, target: 3DHP\n",
        "4.   Cross-dataset trainin on 3DPW: source H3.6M, target: 3DPW\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1SLaOn0PIfmT"
      },
      "source": [
        "# Install Dependencies\n",
        "\n",
        "请用python3.8环境，以确保能找到对应pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 临时补充\n",
        "# %pip install pexpect\n",
        "# %pip install psutil\n",
        "%pip install PyQt5\n",
        "# %pip install distutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WePRpLEMZK-O",
        "outputId": "10a5d497-3fe3-4b21-db75-739552ef07d0"
      },
      "outputs": [],
      "source": [
        "%pip install certifi==2020.12.5\n",
        "%pip install cffi==1.14.0\n",
        "%pip install cycler==0.10.0\n",
        "%pip install kiwisolver==1.3.1\n",
        "%pip install matplotlib==3.1.3\n",
        "# %pip install mkl-fft==1.3.0\n",
        "# pip上只有很老的版本，需要conda安装\n",
        "%conda install -c intel mkl_fft==1.3.0\n",
        "%pip install mkl-random==1.1.1\n",
        "%pip install mkl-service==2.3.0\n",
        "%pip install nbconvert==5.6.1\n",
        "%pip install numpy \n",
        "%pip install olefile==0.46\n",
        "%pip install Pillow \n",
        "%pip install protobuf==3.15.6\n",
        "%pip install pycparser \n",
        "%pip install pyparsing==2.4.7\n",
        "%pip install python-dateutil==2.8.1\n",
        "%pip install scipy==1.4.1\n",
        "%pip install six \n",
        "%pip install tensorboardX==1.6\n",
        "# %pip install torch\n",
        "%conda install pytorch torchvision pytorch-cuda=11.6 -c pytorch -c nvidia\n",
        "%pip install torchgeometry==0.1.2\n",
        "# %pip install torchvision==0.2.2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pytorch安装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SGJ-DPf5SrJG",
        "outputId": "fa53dc6f-34b6-4e02-ab5a-fb462a2ddae5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "version_str=\"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace(\".\",\"\"),\n",
        "    f\"_pyt{torch.__version__[0:5:2]}\"\n",
        "])\n",
        "%pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-hFL4HUhV2tI"
      },
      "source": [
        "# 下载数据\n",
        "\n",
        "实验室服务器连不到google，需要手动下载后上传"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1SEly7kV27E"
      },
      "outputs": [],
      "source": [
        "# #%cd /TO/AdaptPose/ \n",
        "# !ls\n",
        "# %cd data\n",
        "# ## download data\n",
        "# !gdown --id 1_cJ2vG5wxRQUtWavz47XAwaMvU9pQB_X\n",
        "# !gdown --id 1NX9dT1nKX-8t_eZR3vnfrdyOegG7pDBs\n",
        "# !gdown --id 1v3-LDnjlHT8OJi40E7OKDBGY7dWM9P2n\n",
        "# !gdown --id 12voJG5DBFy_2xT3lNdLYA4ZCTYjs7Iyv\n",
        "# !gdown --id 1Y96GlQOEkc2Gx6V4FpvTwCyJjrUX3FYu\n",
        "# !gdown --id 1vL53iJ1mWao3TKK3p52ZX_jdF5d5EBtl\n",
        "# !gdown --id 1EbVV-nzrrQ2KdT_GG4JPwv1A7EOnFka1\n",
        "# ## download pretrained models\n",
        "# %cd ..\n",
        "# %cd checkpoint/adaptpose/videopose/gt/3dhp/\n",
        "# !gdown --id 11QJf-B5D1_aqsMR3v-9rrfht4W7ErC4k\n",
        "# %cd ../../../../..\n",
        "# %cd checkpoint/adaptpose/videopose/gt/3dpw/\n",
        "# !gdown --id 1LUi1OQ7vWQ5KXJ9HlSvtveiOEacG1WoS\n",
        "# %cd ../../../../..\n",
        "# %cd checkpoint/adaptpose/videopose/gt/skii/\n",
        "# !gdown --id 1b1_A7SuBdGaPNXzsx2qLOg0_RqIuk6Cx\n",
        "# %cd ../../../../..\n",
        "# %cd checkpoint/pretrain_baseline/videopose/gt/3dhp/\n",
        "# !gdown --id 1SmSXrk_LcpsXBpBEsSp0zq9qIpb4aJBU\n",
        "# %cd ../../../../.."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M_gEjI73LAES"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TIoGNrmTbr5G"
      },
      "source": [
        "## 1. 作者提供的3DHP迁移后最佳表现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWHxp3hWcMfH",
        "outputId": "a0a14ce3-9cfc-4512-9065-35aec3f42efe"
      },
      "outputs": [],
      "source": [
        "!python3  run_evaluate.py --posenet_name 'videopose' --keypoints gt --evaluate  'checkpoint/adaptpose/videopose/gt/3dhp/ckpt_best_dhp_p1.pth.tar' --dataset_target 3dhp --keypoints_target 'gt' --pad 13  --pretrain_path  'checkpoint/pretrain_baseline/videopose/gt/3dhp/ckpt_best.pth.tar'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gE8hENgkWhvt"
      },
      "source": [
        "## 2. 对3DHP做迁移训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cb3ckdkanDJ1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using settings Namespace(actions='*', ba_range_m=0.205, ba_range_w=0.165, batch_size=1024, blr_limit=0.1, blr_tanhlimit=0.2, checkpoint='./checkpoint/adaptpose', dataset='h36m', dataset_target='3dhp', decay_epoch=0, df=2, downsample=1, dropout=0.25, epochs=20, evaluate='', gloss_factorblr=1, gloss_factord2d=1, gloss_factord2d_temp=1, gloss_factord3d=6, gloss_factordiff=3, gloss_factordiv_ba=0.0, gloss_factordiv_rt=0.0, gloss_factorfeedback=0.001, gloss_factorfeedback_ba=0.1, gloss_factorfeedback_rt=0.01, hardratio_ba=5, hardratio_ba_s=3, hardratio_rt=17, hardratio_rt_s=17, hardratio_std_ba=2, hardratio_std_rt=15, keypoints='gt', keypoints_target='gt', lr_d=0.0001, lr_g=0.0001, lr_p=0.0001, max_norm=True, note='poseaug', num_workers=2, pad=13, posenet_name='videopose', pretrain=True, pretrain_path='./checkpoint/pretrain_baseline/videopose/gt/3dhp/ckpt_best.pth.tar', random_seed=0, resume='', s1only=False, snapshot=2, stages=4, warmup=2)\n",
            "==> Main script, data_preparation() start ...\n",
            "==> Loading 3D data from Human36mDataset ...\n",
            "==> Loading 2D detections at data/data_2d_h36m_gt.npz ...\n",
            "Generating 1559752 poses...\n",
            "Generating 543344 poses...\n",
            "pose_target_2d shape: (1666994, 16, 2)\n",
            "Generating 1666994 poses...\n",
            "Generating 1559752 poses...\n",
            "Generating 1842302 poses...\n",
            "TS1\n",
            "TS2\n",
            "TS3\n",
            "TS4\n",
            "TS5\n",
            "TS6\n",
            "Loaded target dataset 3dhp/h36m, test_shape: (24688, 16, 3)\n",
            "Generating 24688 poses...\n",
            "==> Main script, data_preparation() done ...\n",
            "==> Main script, Creating PoseNet model start ...\n",
            "create model: videopose\n",
            "==> Total parameters for model videopose: 8.54M\n",
            "==> Pretrained posenet loaded\n",
            "create model: videopose\n",
            "==> Total parameters for model videopose: 8.54M\n",
            "==> Pretrained posenet loaded\n",
            "==> Main script, Creating PoseAug model start ...\n",
            "==> Creating model...\n",
            "==> Total parameters: 1.18M\n",
            "==> Total parameters: 1.08M\n",
            "==> Total parameters: 0.04M\n",
            "==> Total parameters: 0.82M\n",
            "==> Main script, Creating PoseAug model done ...\n",
            "==> Main script, Making checkpoint dir: ./checkpoint/adaptpose/videopose/gt/2023-02-09T16:38:05.178824_poseaug\n",
            "\u001b[KEval posenet on H36M_test |################################| (531/531) Data: 0.008127s | Batch: 0.041s | Total: 0:00:21 | ETA: 0:00:01 | MPJPE:  41.4052 | P-MPJPE:  30.6925 | N-MPJPE:  38.0633al posenet on H36M_test |                                | (9/531) Data: 0.043158s | Batch: 0.281s | Total: 0:00:02 | ETA: 0:02:43 | MPJPE:  43.7612 | P-MPJPE:  31.3910 | N-MPJPE:  40.0828\n",
            "\u001b[KEval posenet on mpi3d_loader |################################| (25/25) Data: 0.014618s | Batch: 0.085s | Total: 0:00:02 | ETA: 0:00:01 | MPJPE:  96.4169 | P-MPJPE:  66.4824 | N-MPJPE:  89.2948\n",
            "\u001b[?25h\u001b[?25lTraceback (most recent call last):\n",
            "  File \"run_adaptpose.py\", line 185, in <module>\n",
            "    main(args)\n",
            "  File \"run_adaptpose.py\", line 114, in main\n",
            "    fake_3d_sample, fake_2d_sample, summary, writer, section=kk)\n",
            "  File \"/home/data/yyc/ap/function_adaptpose/model_gan_train.py\", line 323, in train_gan\n",
            "    g_rlt = model_G(inputs_3d_random,target_d2d)\n",
            "  File \"/home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/data/yyc/ap/models_adaptpose/gan_generator.py\", line 67, in forward\n",
            "    pose_bl, blr = self.BLprocess(inputs_3d, pose_ba)  # blr used for debug\n",
            "  File \"/home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/data/yyc/ap/models_adaptpose/gan_generator.py\", line 374, in forward\n",
            "    augx_bl = blaugment9to15(augx, bones_length, blr.unsqueeze(3))\n",
            "  File \"/home/data/yyc/ap/utils/gan_utils.py\", line 40, in blaugment9to15\n",
            "    bones_unit = get_bone_unit_vecbypose3d(x)\n",
            "  File \"/home/data/yyc/ap/utils/gan_utils.py\", line 148, in get_bone_unit_vecbypose3d\n",
            "    bonelength = get_bone_lengthbypose3d(x)\n",
            "  File \"/home/data/yyc/ap/utils/gan_utils.py\", line 139, in get_bone_lengthbypose3d\n",
            "    bonevec = get_BoneVecbypose3d(x)\n",
            "  File \"/home/data/yyc/ap/utils/gan_utils.py\", line 122, in get_BoneVecbypose3d\n",
            "    B = torch.matmul(pose3, C)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 287.89 MiB already allocated; 3.81 MiB free; 294.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ],
      "source": [
        "epoch_num=20\n",
        "# nohup python3 run_adaptpose.py --note poseaug --posenet_name 'videopose' --lr_p 1e-4 --checkpoint './checkpoint/adaptpose' --keypoints gt --keypoints_target gt --dataset_target '3dhp'  --pretrain_path './checkpoint/pretrain_baseline/videopose/gt/3dhp/ckpt_best.pth.tar'  --pad 13 >>train.log 2>&1 &\n",
        "# notebook不支持后台运行，copy上面这行手动跑\n",
        "!python3 run_adaptpose.py --note poseaug --posenet_name 'videopose' --lr_p 1e-4 --checkpoint './checkpoint/adaptpose' --keypoints gt --keypoints_target gt --dataset_target '3dhp'  --pretrain_path './checkpoint/pretrain_baseline/videopose/gt/3dhp/ckpt_best.pth.tar'  --pad 13 --epochs {epoch_num}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 在3DHP上测试未经迁移的模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3  run_evaluate.py --posenet_name 'videopose' --keypoints gt --evaluate 'checkpoint/pretrain_baseline/videopose/gt/3dhp/ckpt_best.pth.tar' --dataset_target 3dhp --keypoints_target 'gt' --pad 13"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 在3DHP上测试复现训练的模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using settings Namespace(actions='*', ba_range_m=0.205, ba_range_w=0.165, batch_size=1024, blr_limit=0.1, blr_tanhlimit=0.2, checkpoint='checkpoint/debug', dataset='h36m', dataset_target='3dhp', decay_epoch=0, df=2, downsample=1, dropout=0.25, epochs=20, evaluate='checkpoint/adaptpose/videopose/gt/trained3dhp2/ckpt_best_dhp_p1.pth.tar', gloss_factorblr=1, gloss_factord2d=1, gloss_factord2d_temp=1, gloss_factord3d=6, gloss_factordiff=3, gloss_factordiv_ba=0.0, gloss_factordiv_rt=0.0, gloss_factorfeedback=0.001, gloss_factorfeedback_ba=0.1, gloss_factorfeedback_rt=0.01, hardratio_ba=5, hardratio_ba_s=3, hardratio_rt=17, hardratio_rt_s=17, hardratio_std_ba=2, hardratio_std_rt=15, keypoints='gt', keypoints_target='gt', lr_d=0.0001, lr_g=0.0001, lr_p=0.0001, max_norm=True, note='debug', num_workers=2, pad=13, posenet_name='videopose', pretrain=False, pretrain_path='', random_seed=0, resume='', s1only=False, snapshot=2, stages=4, warmup=2)\n",
            "==> Loading dataset...\n",
            "==> Loading 3D data from Human36mDataset ...\n",
            "==> Loading 2D detections at data/data_2d_h36m_gt.npz ...\n",
            "Generating 1559752 poses...\n",
            "Generating 543344 poses...\n",
            "pose_target_2d shape: (1666994, 16, 2)\n",
            "Generating 1666994 poses...\n",
            "Generating 1559752 poses...\n",
            "Generating 1842302 poses...\n",
            "TS1\n",
            "TS2\n",
            "TS3\n",
            "TS4\n",
            "TS5\n",
            "TS6\n",
            "Loaded target dataset 3dhp/h36m, test_shape: (24688, 16, 3)\n",
            "Generating 24688 poses...\n",
            "==> Creating model...\n",
            "create model: videopose\n",
            "==> Total parameters for model videopose: 8.54M\n",
            "==> Loading checkpoint 'checkpoint/adaptpose/videopose/gt/trained3dhp2/ckpt_best_dhp_p1.pth.tar'\n",
            "==> Evaluating...\n",
            "\u001b[KEval posenet on  |################################| (25/25) Data: 0.016765s | Batch: 0.171s | Total: 0:00:04 | ETA: 0:00:01 | MPJPE:  77.8510 | P-MPJPE:  52.3760 | N-MPJPE:  71.7846\n",
            "测试集: Protocol #1   (MPJPE) overall average: 77.85 (mm)\n",
            "测试集: Protocol #2 (P-MPJPE) overall average: 52.38 (mm)\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "test_folder_name=\"trained3dhp2\"\n",
        "!python3  run_evaluate.py --posenet_name 'videopose' --keypoints gt --evaluate  'checkpoint/adaptpose/videopose/gt/{test_folder_name}/ckpt_best_dhp_p1.pth.tar' --dataset_target '3dhp' --keypoints_target 'gt' --pad 13"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.在3DHP上测试迁移模型，并用自定义的统计方式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_folder_name=\"trained3dhp2\"\n",
        "!python3  run_evaluate_part.py --evaluate  'checkpoint/adaptpose/videopose/gt/{test_folder_name}/ckpt_best_dhp_p1.pth.tar'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOlyqap6yX3jY+F0YChyyZZ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "AdaptPose.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ap",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "b587e32664cc3c61e772aadf20bd8c3a9acfc6bc0e8d288ed704a2bf816be388"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
