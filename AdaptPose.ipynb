{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgholamikn/AdaptPose/blob/main/AdaptPose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtvy95RHLbrd"
      },
      "source": [
        "# **AdaptPose: Cross-dataset Adaptation of 3D Human Pose Estimation by Learnable Motion Generator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hfK6GfCJ5r4"
      },
      "source": [
        "Here we provide the following experiments: \n",
        "\n",
        "1.   Cross-dataset evaluation on 3DHP: source H3.6M, target: 3DHP\n",
        "2.   Cross-dataset evaluation on 3DPW: source H3.6M, target: 3DPW\n",
        "3.   Cross-dataset trainin on 3DHP: source H3.6M, target: 3DHP\n",
        "4.   Cross-dataset trainin on 3DPW: source H3.6M, target: 3DPW\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1SLaOn0PIfmT"
      },
      "source": [
        "# Install Dependencies\n",
        "\n",
        "请用python3.8环境，以确保能找到对应pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 临时补充\n",
        "# %pip install pexpect\n",
        "# %pip install psutil\n",
        "# %pip install PyQt5\n",
        "# %pip install distutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WePRpLEMZK-O",
        "outputId": "10a5d497-3fe3-4b21-db75-739552ef07d0"
      },
      "outputs": [],
      "source": [
        "%pip install certifi==2020.12.5\n",
        "%pip install cffi==1.14.0\n",
        "%pip install cycler==0.10.0\n",
        "%pip install kiwisolver==1.3.1\n",
        "%pip install matplotlib==3.1.3\n",
        "# %pip install mkl-fft==1.3.0\n",
        "# pip上只有很老的版本，需要conda安装\n",
        "%conda install -c intel mkl_fft==1.3.0\n",
        "%pip install mkl-random==1.1.1\n",
        "%pip install mkl-service==2.3.0\n",
        "%pip install nbconvert==5.6.1\n",
        "%pip install numpy \n",
        "%pip install olefile==0.46\n",
        "%pip install Pillow \n",
        "%pip install protobuf==3.15.6\n",
        "%pip install pycparser \n",
        "%pip install pyparsing==2.4.7\n",
        "%pip install python-dateutil==2.8.1\n",
        "%pip install scipy==1.4.1\n",
        "%pip install six \n",
        "%pip install tensorboardX==1.6\n",
        "# %pip install torch\n",
        "%conda install pytorch torchvision pytorch-cuda=11.6 -c pytorch -c nvidia\n",
        "%pip install torchgeometry==0.1.2\n",
        "# %pip install torchvision==0.2.2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pytorch安装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SGJ-DPf5SrJG",
        "outputId": "fa53dc6f-34b6-4e02-ab5a-fb462a2ddae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py36_cu102_pyt11./download.html\n",
            "Requirement already satisfied: pytorch3d in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (0.3.0)\n",
            "Requirement already satisfied: fvcore in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from pytorch3d) (0.1.5.post20221221)\n",
            "Requirement already satisfied: torchvision>=0.4 in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from pytorch3d) (0.11.3)\n",
            "Requirement already satisfied: numpy in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from torchvision>=0.4->pytorch3d) (1.19.5)\n",
            "Requirement already satisfied: torch in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from torchvision>=0.4->pytorch3d) (1.10.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from torchvision>=0.4->pytorch3d) (8.4.0)\n",
            "Requirement already satisfied: tqdm in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from fvcore->pytorch3d) (4.19.9)\n",
            "Requirement already satisfied: termcolor>=1.1 in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from fvcore->pytorch3d) (1.1.0)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from fvcore->pytorch3d) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from fvcore->pytorch3d) (0.8.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from fvcore->pytorch3d) (6.0)\n",
            "Requirement already satisfied: dataclasses in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from fvcore->pytorch3d) (0.8)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from fvcore->pytorch3d) (0.1.10)\n",
            "Requirement already satisfied: portalocker in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from iopath>=0.1.7->fvcore->pytorch3d) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions in /home/data/yyc/.conda/envs/ap/lib/python3.6/site-packages (from iopath>=0.1.7->fvcore->pytorch3d) (4.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "version_str=\"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace(\".\",\"\"),\n",
        "    f\"_pyt{torch.__version__[0:5:2]}\"\n",
        "])\n",
        "%pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-hFL4HUhV2tI"
      },
      "source": [
        "# 下载数据\n",
        "\n",
        "实验室服务器连不到google，需要手动下载后上传"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1SEly7kV27E"
      },
      "outputs": [],
      "source": [
        "# #%cd /TO/AdaptPose/ \n",
        "# !ls\n",
        "# %cd data\n",
        "# ## download data\n",
        "# !gdown --id 1_cJ2vG5wxRQUtWavz47XAwaMvU9pQB_X\n",
        "# !gdown --id 1NX9dT1nKX-8t_eZR3vnfrdyOegG7pDBs\n",
        "# !gdown --id 1v3-LDnjlHT8OJi40E7OKDBGY7dWM9P2n\n",
        "# !gdown --id 12voJG5DBFy_2xT3lNdLYA4ZCTYjs7Iyv\n",
        "# !gdown --id 1Y96GlQOEkc2Gx6V4FpvTwCyJjrUX3FYu\n",
        "# !gdown --id 1vL53iJ1mWao3TKK3p52ZX_jdF5d5EBtl\n",
        "# !gdown --id 1EbVV-nzrrQ2KdT_GG4JPwv1A7EOnFka1\n",
        "# ## download pretrained models\n",
        "# %cd ..\n",
        "# %cd checkpoint/adaptpose/videopose/gt/3dhp/\n",
        "# !gdown --id 11QJf-B5D1_aqsMR3v-9rrfht4W7ErC4k\n",
        "# %cd ../../../../..\n",
        "# %cd checkpoint/adaptpose/videopose/gt/3dpw/\n",
        "# !gdown --id 1LUi1OQ7vWQ5KXJ9HlSvtveiOEacG1WoS\n",
        "# %cd ../../../../..\n",
        "# %cd checkpoint/adaptpose/videopose/gt/skii/\n",
        "# !gdown --id 1b1_A7SuBdGaPNXzsx2qLOg0_RqIuk6Cx\n",
        "# %cd ../../../../..\n",
        "# %cd checkpoint/pretrain_baseline/videopose/gt/3dhp/\n",
        "# !gdown --id 1SmSXrk_LcpsXBpBEsSp0zq9qIpb4aJBU\n",
        "# %cd ../../../../.."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M_gEjI73LAES"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TIoGNrmTbr5G"
      },
      "source": [
        "## 1. Cross-dataset Evaluation of Pretrained Model on 3DHP dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWHxp3hWcMfH",
        "outputId": "a0a14ce3-9cfc-4512-9065-35aec3f42efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using settings Namespace(actions='*', ba_range_m=0.205, ba_range_w=0.165, batch_size=1024, blr_limit=0.1, blr_tanhlimit=0.2, checkpoint='checkpoint/debug', dataset='h36m', dataset_target='3dhp', decay_epoch=0, df=2, downsample=1, dropout=0.25, epochs=50, evaluate='checkpoint/adaptpose/videopose/gt/3dhp/ckpt_best_dhp_p1.pth.tar', gloss_factorblr=1, gloss_factord2d=1, gloss_factord2d_temp=1, gloss_factord3d=6, gloss_factordiff=3, gloss_factordiv_ba=0.0, gloss_factordiv_rt=0.0, gloss_factorfeedback=0.001, gloss_factorfeedback_ba=0.1, gloss_factorfeedback_rt=0.01, hardratio_ba=5, hardratio_ba_s=3, hardratio_rt=17, hardratio_rt_s=17, hardratio_std_ba=2, hardratio_std_rt=15, keypoints='gt', keypoints_target='gt', lr_d=0.0001, lr_g=0.0001, lr_p=0.0001, max_norm=True, note='debug', num_workers=2, pad=13, posenet_name='videopose', pretrain=False, pretrain_path='checkpoint/pretrain_baseline/videopose/gt/3dhp/ckpt_best.pth.tar', random_seed=0, resume='', s1only=False, snapshot=2, stages=4, warmup=2)\n",
            "==> Loading dataset...\n",
            "==> Loading 3D data...\n",
            "==> Loading 2D detections...\n",
            "Generating 1559752 poses...\n",
            "Generating 543344 poses...\n",
            "target_shape (1666994, 16, 2)\n",
            "Generating 1666994 poses...\n",
            "Generating 1559752 poses...\n",
            "Generating 1842302 poses...\n",
            "TS1\n",
            "TS2\n",
            "TS3\n",
            "TS4\n",
            "TS5\n",
            "TS6\n",
            "test_shape (24688, 16, 3)\n",
            "Generating 24688 poses...\n",
            "==> Creating model...\n",
            "create model: videopose\n",
            "==> Total parameters for model videopose: 8.54M\n",
            "==> Loading checkpoint 'checkpoint/adaptpose/videopose/gt/3dhp/ckpt_best_dhp_p1.pth.tar'\n",
            "==> Evaluating...\n",
            "\u001b[KEval posenet on  |################################| (25/25) Data: 0.024045s | Batch: 0.175s | Total: 0:00:04 | ETA: 0:00:01 | MPJPE:  77.0824 | P-MPJPE:  52.8427 | N-MPJPE:  71.3439\n",
            "3DHP: Protocol #1   (MPJPE) overall average: 77.08 (mm)\n",
            "3DHP: Protocol #2 (P-MPJPE) overall average: 52.84 (mm)\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!python3  run_evaluate.py --posenet_name 'videopose' --keypoints gt --evaluate  'checkpoint/adaptpose/videopose/gt/3dhp/ckpt_best_dhp_p1.pth.tar' --dataset_target 3dhp --keypoints_target 'gt' --pad 13  --pretrain_path  'checkpoint/pretrain_baseline/videopose/gt/3dhp/ckpt_best.pth.tar'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pD7B0vxNcUd9"
      },
      "source": [
        "## 2. Cross-dataset Evaluation of Pretrained Model on 3DPW dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfxWubT6cY5Y",
        "outputId": "2ae3f1a6-4b3d-4b6e-9225-c1c91120dd3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using settings Namespace(actions='*', ba_range_m=0.205, ba_range_w=0.165, batch_size=1024, blr_limit=0.1, blr_tanhlimit=0.2, checkpoint='checkpoint/debug', dataset='h36m', dataset_target='3dpw', decay_epoch=0, df=2, downsample=1, dropout=0.25, epochs=50, evaluate='checkpoint/adaptpose/videopose/gt/3dpw/ckpt_best_dhp_p1.pth.tar', gloss_factorblr=1, gloss_factord2d=1, gloss_factord2d_temp=1, gloss_factord3d=6, gloss_factordiff=3, gloss_factordiv_ba=0.0, gloss_factordiv_rt=0.0, gloss_factorfeedback=0.001, gloss_factorfeedback_ba=0.1, gloss_factorfeedback_rt=0.01, hardratio_ba=5, hardratio_ba_s=3, hardratio_rt=17, hardratio_rt_s=17, hardratio_std_ba=2, hardratio_std_rt=15, keypoints='gt', keypoints_target='gt', lr_d=0.0001, lr_g=0.0001, lr_p=0.0001, max_norm=True, note='debug', num_workers=2, pad=13, posenet_name='videopose', pretrain=False, pretrain_path='checkpoint/pretrain_baseline/videopose/gt/3dpw/ckpt_best.pth.tar', random_seed=0, resume='', s1only=False, snapshot=2, stages=4, warmup=2)\n",
            "==> Loading dataset...\n",
            "==> Loading 3D data...\n",
            "==> Loading 2D detections...\n",
            "Generating 1559752 poses...\n",
            "Generating 543344 poses...\n",
            "3dpw_shape (1643300, 16, 3)\n",
            "target_shape (1643300, 16, 2)\n",
            "Generating 1643300 poses...\n",
            "Generating 1559752 poses...\n",
            "Generating 1643300 poses...\n",
            "test_shape (37154, 16, 2)\n",
            "Generating 37154 poses...\n",
            "==> Creating model...\n",
            "create model: videopose\n",
            "==> Total parameters for model videopose: 8.54M\n",
            "==> Loading checkpoint 'checkpoint/adaptpose/videopose/gt/3dpw/ckpt_best_dhp_p1.pth.tar'\n",
            "==> Evaluating...\n",
            "\u001b[KEval posenet on  |################################| (37/37) Data: 0.016306s | Batch: 0.136s | Total: 0:00:05 | ETA: 0:00:01 | MPJPE:  99.3302 | P-MPJPE:  62.0375 | N-MPJPE:  95.637303\n",
            "3DHP: Protocol #1   (MPJPE) overall average: 99.33 (mm)\n",
            "3DHP: Protocol #2 (P-MPJPE) overall average: 62.04 (mm)\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!python3  run_evaluate.py --posenet_name 'videopose' --keypoints gt --evaluate  'checkpoint/adaptpose/videopose/gt/3dpw/ckpt_best_dhp_p1.pth.tar' --dataset_target 3dpw --keypoints_target 'gt' --pad 13  --pretrain_path  'checkpoint/pretrain_baseline/videopose/gt/3dpw/ckpt_best.pth.tar'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gE8hENgkWhvt"
      },
      "source": [
        "## 3. Cross-dataset Training for 3DHP\n",
        "\n",
        "根据https://stackoverflow.com/questions/56129786/cannot-load-backend-qt5agg-which-requires-the-qt5-interactive-framework-as\n",
        "\n",
        "在notebook中运行会报错，需要直接运行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.rcsetup.interactive_bk # 获取 interactive backend\n",
        "matplotlib.rcsetup.non_interactive_bk # 获取 non-interactive backend\n",
        "matplotlib.rcsetup.all_backends # 获取 所有 backend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb3ckdkanDJ1"
      },
      "outputs": [],
      "source": [
        "!python3 run_adaptpose.py --note poseaug --posenet_name 'videopose' --lr_p 1e-4 --checkpoint './checkpoint/adaptpose' --keypoints gt --keypoints_target gt --dataset_target '3dhp'  --pretrain_path './checkpoint/pretrain_baseline/videopose/gt/3dhp/ckpt_best.pth.tar'  --pad 13 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RAGIz3qbJRqb"
      },
      "source": [
        "## 4. Cross-dataset Training of Pretrained Model on 3DPW dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMK7xXrUJv_I"
      },
      "outputs": [],
      "source": [
        "!python3 run_adaptpose.py --note poseaug --posenet_name 'videopose' --lr_p 1e-4 --checkpoint './checkpoint/adaptpose' --keypoints gt --keypoints_target gt --dataset_target '3dpw'  --pretrain_path './checkpoint/pretrain_baseline/videopose/gt/3dhp/ckpt_best.pth.tar'  --pad 13 "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOlyqap6yX3jY+F0YChyyZZ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "AdaptPose.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ap",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "b587e32664cc3c61e772aadf20bd8c3a9acfc6bc0e8d288ed704a2bf816be388"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
